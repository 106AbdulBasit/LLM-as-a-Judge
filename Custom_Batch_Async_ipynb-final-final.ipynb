{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWx-odI03oNm",
        "outputId": "78ddc792-f218-44f8-879d-0835dfe4c55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# installing Libraries\n",
        "%pip install anthropic openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libaries"
      ],
      "metadata": {
        "id": "SRSylv_i33Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "import anthropic\n",
        "import queue\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "id": "Xf_EcXPu3_mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0429f51f-3696-458e-b8c6-ac17a10783c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseJudge(ABC):\n",
        "    \"\"\"Abstract base class for instruction-following judges.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    @abstractmethod\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"\n",
        "        Evaluate how many instructions were followed exactly.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of editing instructions to evaluate\n",
        "            original_paragraph: The original text before editing\n",
        "            completion: The edited text to evaluate\n",
        "\n",
        "        Returns:\n",
        "            Number of instructions followed exactly (0 to len(instructions))\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_evaluation_prompt(self, instructions: List[str], original_paragraph: str, completion: str) -> Tuple[str, str]:\n",
        "        \"\"\"Create system and user prompts for evaluation.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert evaluator of instruction-following in text editing tasks. \"\n",
        "            \"You will be given a set of editing instructions, the original paragraph, and a modified paragraph. \"\n",
        "            \"You must return ONLY the number of instructions that were followed exactly. \"\n",
        "            \"Do not explain or justify. Just return a single number (0 to N).\"\n",
        "        )\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "            Instructions:\n",
        "            {chr(10).join(f\"{i+1}. {instr}\" for i, instr in enumerate(instructions))}\n",
        "\n",
        "            Original Paragraph:\n",
        "            {original_paragraph}\n",
        "\n",
        "            Edited Output:\n",
        "            {completion}\n",
        "\n",
        "            How many of the above {len(instructions)} instructions were followed exactly?\n",
        "            Return just the number, nothing else.\n",
        "        \"\"\"\n",
        "        return system_prompt, user_prompt\n",
        "\n",
        "    def _extract_score(self, response_text: str, max_instructions: int) -> int:\n",
        "        \"\"\"Extract numerical score from response text.\"\"\"\n",
        "        matched_str = re.search(r'\\d+', response_text.strip())\n",
        "        if matched_str:\n",
        "            value = int(matched_str.group(0))\n",
        "            return max(0, min(value, max_instructions))\n",
        "        else:\n",
        "            print(f\"[{self.name}] Unexpected format: {response_text}\")\n",
        "            return 0"
      ],
      "metadata": {
        "id": "T9Y3X2zG4NBH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class OpenAIJudge(BaseJudge):\n",
        "    \"\"\"OpenAI-based instruction-following judge.\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4\"):\n",
        "        super().__init__(\"OpenAI\")\n",
        "\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "        self.client = AsyncOpenAI(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"Evaluate using OpenAI GPT model.\"\"\"\n",
        "        system_prompt, user_prompt = self._create_evaluation_prompt(instructions, original_paragraph, completion)\n",
        "\n",
        "        try:\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=50\n",
        "            )\n",
        "            reply = response.choices[0].message.content\n",
        "            return self._extract_score(reply, len(instructions))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error: {e}\")\n",
        "            return 0"
      ],
      "metadata": {
        "id": "ekosoWEY4TzN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClaudeJudge(BaseJudge):\n",
        "    \"\"\"Claude-based instruction-following judge.\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"claude-3-5-sonnet-20240620\"):\n",
        "        super().__init__(\"Claude\")\n",
        "        api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "        self.client = anthropic.AsyncAnthropic(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"Evaluate using Claude model.\"\"\"\n",
        "        system_prompt, user_prompt = self._create_evaluation_prompt(instructions, original_paragraph, completion)\n",
        "\n",
        "        try:\n",
        "            response = await self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=50,\n",
        "                temperature=0,\n",
        "                system=system_prompt,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ]\n",
        "            )\n",
        "            reply = response.content[0].text\n",
        "            return self._extract_score(reply, len(instructions))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error: {e}\")\n",
        "            return 0"
      ],
      "metadata": {
        "id": "k39tqJkO4Yl5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionProcessor:\n",
        "    \"\"\"Utility class for processing instructions and prompts.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_instructions_and_paragraph(prompt: str) -> Tuple[List[str], str]:\n",
        "        \"\"\"\n",
        "        Extract instructions and paragraph from a formatted prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: The formatted prompt containing instructions and paragraph\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (instructions_list, paragraph)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If markers are not found or content is empty\n",
        "        \"\"\"\n",
        "        start_marker = \"Instructions:\"\n",
        "        end_marker = \"Return the final output as plain text with line breaks between paragraphs.\"\n",
        "\n",
        "        if start_marker not in prompt or end_marker not in prompt:\n",
        "            raise ValueError(\"Start or end marker not found in prompt.\")\n",
        "\n",
        "        # Find start and end positions\n",
        "        start_index = prompt.index(start_marker) + len(start_marker)\n",
        "        end_index = prompt.index(end_marker) + len(end_marker)\n",
        "\n",
        "        # Extract instruction block\n",
        "        instructions_block = prompt[start_index:prompt.index(end_marker)].strip()\n",
        "\n",
        "        # Split instructions into list by lines and remove empty ones\n",
        "        instructions_list = [line.strip() for line in instructions_block.splitlines() if line.strip()]\n",
        "\n",
        "        # Extract paragraph after end marker\n",
        "        paragraph = prompt[end_index:].strip()\n",
        "\n",
        "        if not instructions_list:\n",
        "            raise ValueError(\"No instructions found between markers.\")\n",
        "        if not paragraph:\n",
        "            raise ValueError(\"No paragraph found after the instructions block.\")\n",
        "\n",
        "        return instructions_list, paragraph\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_instructions(instructions: List[str], n_parts: int = 2) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Split instructions into approximately equal chunks.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of instructions to split\n",
        "            n_parts: Number of parts to split into\n",
        "\n",
        "        Returns:\n",
        "            List of instruction chunks\n",
        "        \"\"\"\n",
        "        total = len(instructions)\n",
        "        base_size = total // n_parts\n",
        "        remainder = total % n_parts\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        for i in range(n_parts):\n",
        "            # First 'remainder' chunks get one extra item\n",
        "            chunk_size = base_size + (1 if i < remainder else 0)\n",
        "            end = start + chunk_size\n",
        "            chunks.append(instructions[start:end])\n",
        "            start = end\n",
        "\n",
        "        return chunks"
      ],
      "metadata": {
        "id": "S9EgpCzZ4hCr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionFollowingEvaluator:\n",
        "\n",
        "    \"\"\"\n",
        "    Main evaluator class that coordinates multiple judges to score instruction-following with batch processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, judges: List[BaseJudge] = None, n_chunks: int = 2,\n",
        "                 batch_size: int = 10, max_concurrent_batches: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            judges: List of judge instances to use for evaluation. If None, auto-creates based on available API keys.\n",
        "            n_chunks: Number of chunks to split instructions into for parallel processing\n",
        "            batch_size: Number of completions to process in each batch\n",
        "            max_concurrent_batches: Maximum number of batches to process concurrently\n",
        "        \"\"\"\n",
        "        self.n_chunks = n_chunks\n",
        "        self.batch_size = batch_size\n",
        "        self.max_concurrent_batches = max_concurrent_batches\n",
        "        self.processor = InstructionProcessor()\n",
        "        self.judges = judges\n",
        "\n",
        "        # output queue\n",
        "        self.q = queue.Queue()\n",
        "\n",
        "    async def _evaluate_single_completion(\n",
        "        self,\n",
        "        instructions: List[str],\n",
        "        paragraph: str,\n",
        "        completion: str,\n",
        "        model_id:str\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        Evaluate a single completion using all judges.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of instructions to evaluate against\n",
        "            paragraph: Original paragraph\n",
        "            completion: Completion to evaluate\n",
        "\n",
        "        Returns:\n",
        "            List of scores from each judge (normalized 0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        # Split instructions into chunks for parallel processing\n",
        "        instruction_chunks = self.processor.chunk_instructions(instructions, self.n_chunks)\n",
        "        total_instructions = len(instructions)\n",
        "\n",
        "        judge_scores = []\n",
        "\n",
        "        # Evaluate with each judge\n",
        "        for judge in self.judges:\n",
        "            # Process chunks in parallel for this judge\n",
        "            chunk_tasks = [\n",
        "                judge.evaluate(chunk, paragraph, completion)\n",
        "                for chunk in instruction_chunks\n",
        "            ]\n",
        "\n",
        "            chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)\n",
        "\n",
        "            # Sum up followed instructions across chunks\n",
        "            total_followed = 0\n",
        "            for result in chunk_results:\n",
        "                if isinstance(result, Exception):\n",
        "                    print(f\"[{judge.name}] Chunk evaluation failed: {result}\")\n",
        "                    continue\n",
        "                total_followed += result\n",
        "\n",
        "            # Normalize score\n",
        "            normalized_score = total_followed / total_instructions if total_instructions > 0 else 0.0\n",
        "            self.q.put([judge.name, model_id, normalized_score])\n",
        "            judge_scores.append(normalized_score)\n",
        "\n",
        "        return judge_scores\n",
        "\n",
        "    async def _process_batch(\n",
        "        self,\n",
        "        batch_data: List[Tuple[str, str, str]],\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Process a batch of completions asynchronously.\n",
        "\n",
        "        Args:\n",
        "            batch_data: List of (prompt, completion, model_id) tuples\n",
        "            instructions: List of instructions to evaluate against\n",
        "            paragraph: Original paragraph\n",
        "\n",
        "        Returns:\n",
        "            List of (model_id, score) tuples\n",
        "        \"\"\"\n",
        "        batch_results = []\n",
        "\n",
        "        # Create tasks for all completions in this batch\n",
        "        evaluation_tasks = []\n",
        "        for obj in batch_data:\n",
        "            instructions, paragraph = self.processor.extract_instructions_and_paragraph(obj[0])\n",
        "            task = self._evaluate_single_completion(instructions, paragraph, obj[1], obj[2])\n",
        "            evaluation_tasks.append((task, obj[2]))\n",
        "\n",
        "        print(f\"Processing batch of {len(evaluation_tasks)} completions...\")\n",
        "\n",
        "        # Execute all evaluations in the batch concurrently\n",
        "        for task, model_id in evaluation_tasks:\n",
        "            try:\n",
        "                judge_scores = await task\n",
        "\n",
        "                # Average across judges for this completion\n",
        "                if judge_scores:\n",
        "                    final_score = sum(judge_scores) / len(judge_scores)\n",
        "                    batch_results.append((model_id, final_score))\n",
        "                else:\n",
        "                    print(f\"No valid scores for model {model_id}\")\n",
        "                    batch_results.append((model_id, 0.0))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation failed for model {model_id}: {e}\")\n",
        "                batch_results.append((model_id, 0.0))\n",
        "\n",
        "        return batch_results\n",
        "\n",
        "    def _create_batches(self, data: List[Tuple[str, str, str]]) -> List[List[Tuple[str, str, str]]]:\n",
        "        \"\"\"Create batches from the input data.\"\"\"\n",
        "        batches = []\n",
        "        for i in range(0, len(data), self.batch_size):\n",
        "            batch = data[i:i + self.batch_size]\n",
        "            batches.append(batch)\n",
        "        return batches\n",
        "\n",
        "    async def score(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        completions: List[str],\n",
        "        models: List[str]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Score prompt-completion pairs using multiple judges with batch processing.\n",
        "\n",
        "        Args:\n",
        "            prompts: List of full prompts (including instructions + paragraph)\n",
        "            completions: List of completions corresponding to the prompts\n",
        "            models: List of model identifiers for each completion\n",
        "\n",
        "        Returns:\n",
        "            Dictionary: model_id → average compliance score (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        if not (len(prompts) == len(completions) == len(models)):\n",
        "            raise ValueError(\"Input lists must be equal length\")\n",
        "\n",
        "        if not prompts:\n",
        "            return {}\n",
        "\n",
        "        # Prepare data for batch processing\n",
        "        data = list(zip(prompts, completions, models))\n",
        "        batches = self._create_batches(data)\n",
        "\n",
        "        print(f\"Processing {len(data)} completions in {len(batches)} batches...\")\n",
        "\n",
        "        model_scores = defaultdict(list)\n",
        "\n",
        "        # Process batches with concurrency control\n",
        "        semaphore = asyncio.Semaphore(self.max_concurrent_batches)\n",
        "\n",
        "        async def process_batch_with_semaphore(batch):\n",
        "            async with semaphore:\n",
        "                return await self._process_batch(batch)\n",
        "\n",
        "        # Create batch processing tasks\n",
        "        batch_tasks = [process_batch_with_semaphore(batch) for batch in batches]\n",
        "\n",
        "        # Process all batches\n",
        "        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)\n",
        "\n",
        "        # Collect results from all batches\n",
        "        for batch_result in batch_results:\n",
        "            if isinstance(batch_result, Exception):\n",
        "                print(f\"Batch processing failed: {batch_result}\")\n",
        "                continue\n",
        "\n",
        "            for model_id, score in batch_result:\n",
        "                model_scores[model_id].append(score)\n",
        "\n",
        "        # Average scores per model\n",
        "        avg_scores = {\n",
        "            model_id: sum(scores) / len(scores) if scores else 0.0\n",
        "            for model_id, scores in model_scores.items()\n",
        "        }\n",
        "\n",
        "        # Extract elements and create dictionary\n",
        "        scored_list = []\n",
        "        while not self.q.empty():\n",
        "            judge_name, model_id, score = self.q.get()\n",
        "            scored_list.append({\"judge_name\":judge_name, \"model_id\":model_id, \"score\":score})\n",
        "\n",
        "        with open(\"./scored_output.json\", \"w\") as f:\n",
        "            json.dump(scored_list, f)\n",
        "\n",
        "        return avg_scores"
      ],
      "metadata": {
        "id": "rl6LbEd445Jr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def create_evaluator(\n",
        "    batch_size: int = 10,\n",
        "    max_concurrent_batches: int = 3\n",
        ") -> InstructionFollowingEvaluator:\n",
        "    \"\"\"\n",
        "    Create an evaluator with specified judges and flexible API key configuration.\n",
        "\n",
        "    Args:\n",
        "        openai_api_key: OpenAI API key (overrides other sources)\n",
        "        claude_api_key: Claude API key (overrides other sources)\n",
        "        use_openai: Whether to include OpenAI judge\n",
        "        use_claude: Whether to include Claude judge\n",
        "        config_file: Path to JSON config file with API keys\n",
        "        api_config: Dictionary with API key configuration\n",
        "        batch_size: Number of completions to process in each batch\n",
        "        max_concurrent_batches: Maximum number of batches to process concurrently\n",
        "\n",
        "    Returns:\n",
        "        Configured InstructionFollowingEvaluator instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Create judges based on preferences and availability\n",
        "    judges = []\n",
        "    judges.append(OpenAIJudge())\n",
        "    judges.append(ClaudeJudge())\n",
        "\n",
        "    return InstructionFollowingEvaluator(judges, batch_size=batch_size,\n",
        "                                       max_concurrent_batches=max_concurrent_batches)"
      ],
      "metadata": {
        "id": "98g_9o-d458J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_completions(\n",
        "    prompts: List[str],\n",
        "    completions: List[str],\n",
        "    models: List[str],\n",
        "    openai_api_key: Optional[str] = None,\n",
        "    claude_api_key: Optional[str] = None,\n",
        "    config_file: Optional[str] = None,\n",
        "    batch_size: int = 10,\n",
        "    max_concurrent_batches: int = 3\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Convenience function to evaluate completions with flexible API key configuration and batch processing.\n",
        "\n",
        "    Args:\n",
        "        prompts: List of prompts\n",
        "        completions: List of completions\n",
        "        models: List of model identifiers\n",
        "        openai_api_key: OpenAI API key\n",
        "        claude_api_key: Claude API key\n",
        "        config_file: Path to config file with API keys\n",
        "        batch_size: Number of completions to process in each batch\n",
        "        max_concurrent_batches: Maximum number of batches to process concurrently\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of model scores\n",
        "    \"\"\"\n",
        "    evaluator = await create_evaluator(\n",
        "        batch_size=batch_size,\n",
        "        max_concurrent_batches=max_concurrent_batches\n",
        "    )\n",
        "\n",
        "    return await evaluator.score(prompts, completions, models)"
      ],
      "metadata": {
        "id": "hIToX1VK6IJI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def read_prompt_as_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def load_completions(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    completions = re.findall(r\"Completion \\d+:\\s*(.*?)\\s*(?=Completion \\d+:|$)\", text, re.DOTALL)\n",
        "    return [c.strip() for c in completions]\n",
        "\n",
        "def load_models(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        models = [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return models"
      ],
      "metadata": {
        "id": "mLIuvvsm6Qlj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Sample data\n",
        "    prompt = read_prompt_as_text(\"/content/Prompts.txt\")\n",
        "    completions = load_completions(\"/content/Completions.txt\")\n",
        "    models = load_models(\"/content/Models.txt\")\n",
        "    prompts = [prompt] * len(completions)\n",
        "\n",
        "    # Create evaluator with batch processing configuration\n",
        "    evaluator1 = await create_evaluator(\n",
        "        batch_size=5,  # Process 5 completions per batch\n",
        "        max_concurrent_batches=2  # Run maximum 2 batches concurrently\n",
        "    )\n",
        "\n",
        "    # Get scores using batch processing\n",
        "    scores = await evaluator1.score(prompts, completions, models)\n",
        "\n",
        "    print(\"Model Scores:\")\n",
        "    for model_id, score in scores.items():\n",
        "        print(f\"{model_id}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "aKoTMhhV6Tmg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " await main()"
      ],
      "metadata": {
        "id": "1NBdW-Ub6U80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c80d11b-8c60-4c08-b1a1-a7f376b2b856"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 3 completions in 1 batches...\n",
            "Processing batch of 3 completions...\n",
            "Model Scores:\n",
            "Gemini: 0.464\n",
            "Mistral: 0.500\n",
            "OpenAi: 0.464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicual testing"
      ],
      "metadata": {
        "id": "S2GzIpGl7gMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest pytest-asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y91J7WpfwOa",
        "outputId": "dead5599-b3bc-45d7-8eb8-f39e94ecefd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.4.1)\n",
            "Collecting pytest-asyncio\n",
            "  Downloading pytest_asyncio-1.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from pytest) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest) (2.19.2)\n",
            "Downloading pytest_asyncio-1.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytest-asyncio\n",
            "Successfully installed pytest-asyncio-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_test1 = extend_list_by_repeating(prompt1, 90)\n",
        "ompletion_test1 = extend_list_by_repeating(completions, 90)\n",
        "model_test1 = extend_list_by_repeating(models, 90)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTcQNCWWi_3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(model_test1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FipkSuRvX50",
        "outputId": "ce9ac6b1-b1d7-41d4-e99c-649312ad43fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from typing import List, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------- Dummy Mock Judge (returns fixed score) --------\n",
        "class MockJudge:\n",
        "    def __init__(self, fixed_score=3):\n",
        "        self.fixed_score = fixed_score\n",
        "\n",
        "    async def evaluate(self, prompt: str, completion: str, model_id: str) -> Dict[str, float]:\n",
        "        await asyncio.sleep(0.01)  # simulate async delay\n",
        "        return self.fixed_score\n"
      ],
      "metadata": {
        "id": "S-048QdzjPmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_async_execution():\n",
        "    print(\"🔄 Running async execution test...\")\n",
        "\n",
        "    evaluator = InstructionFollowingEvaluator(\n",
        "        judges=[MockJudge(fixed_score=4)],\n",
        "        n_chunks=10,\n",
        "        batch_size=10,\n",
        "        max_concurrent_batches=5,\n",
        "    )\n",
        "\n",
        "    scores = await evaluator.score(prompt_test1, ompletion_test1, model_test1)\n",
        "\n",
        "    assert isinstance(scores, dict), \"❌ Output should be a dictionary\"\n",
        "    assert len(scores) > 0, \"❌ At least one model score should be computed\"\n",
        "    print(\"✅ Async execution test passed!\")\n",
        "    print(scores)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UIlnHaRdjdTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await test_async_execution()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO7vtwyKjye7",
        "outputId": "4217f5d1-2472-48bd-fad0-3d39d8d1bbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Running async execution test...\n",
            "Processing 90 completions in 9 batches...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "Processing batch of 10 completions...\n",
            "✅ Async execution test passed!\n",
            "{'Gemini': 2.8571428571428568, 'Mistral': 2.8571428571428568, 'OpenAi': 2.8571428571428568}\n"
          ]
        }
      ]
    }
  ]
}