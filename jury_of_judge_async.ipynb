{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sppEwZ46Wey6",
        "outputId": "c1558fc8-fd69-4675-f2a8-78e0f344fabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.60.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.60.0-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.1/293.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.60.0\n"
          ]
        }
      ],
      "source": [
        "%pip install anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kjmk9NY1WzWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "uFSXedR6WzXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "import anthropic"
      ],
      "metadata": {
        "id": "qvFkwlW5W3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**class Evaluation Result**"
      ],
      "metadata": {
        "id": "_OlJyMDcW8r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationResult:\n",
        "    \"\"\"Data class to hold evaluation results.\"\"\"\n",
        "    model_id: str\n",
        "    score: float\n",
        "    followed_instructions: int\n",
        "    total_instructions: int"
      ],
      "metadata": {
        "id": "vjTMFdxwXAay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Api Key Manager"
      ],
      "metadata": {
        "id": "znp9zRSuXC4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class APIKeyManager:\n",
        "    \"\"\"Manages API keys for different services with multiple configuration options.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._api_keys = {}\n",
        "\n",
        "    def set_openai_key(self, api_key: str) -> None:\n",
        "        \"\"\"Set OpenAI API key.\"\"\"\n",
        "        self._api_keys['openai'] = api_key\n",
        "        # Also set for openai module\n",
        "        openai.api_key = api_key\n",
        "\n",
        "    def set_claude_key(self, api_key: str) -> None:\n",
        "        \"\"\"Set Claude API key.\"\"\"\n",
        "        self._api_keys['claude'] = api_key\n",
        "\n",
        "    def get_openai_key(self) -> Optional[str]:\n",
        "        \"\"\"Get OpenAI API key with fallback to environment variable.\"\"\"\n",
        "        return self._api_keys.get('openai') or os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    def get_claude_key(self) -> Optional[str]:\n",
        "        \"\"\"Get Claude API key with fallback to environment variable.\"\"\"\n",
        "        return self._api_keys.get('claude') or os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "\n",
        "    def is_openai_configured(self) -> bool:\n",
        "        \"\"\"Check if OpenAI API key is available.\"\"\"\n",
        "        return self.get_openai_key() is not None\n",
        "\n",
        "    def is_claude_configured(self) -> bool:\n",
        "        \"\"\"Check if Claude API key is available.\"\"\"\n",
        "        return self.get_claude_key() is not None\n",
        "\n",
        "    def get_available_services(self) -> List[str]:\n",
        "        \"\"\"Get list of services with configured API keys.\"\"\"\n",
        "        services = []\n",
        "        if self.is_openai_configured():\n",
        "            services.append('openai')\n",
        "        if self.is_claude_configured():\n",
        "            services.append('claude')\n",
        "        return services\n"
      ],
      "metadata": {
        "id": "GV1ya2VNZOsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class BaseJudge"
      ],
      "metadata": {
        "id": "jGa95SUNZ0GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseJudge(ABC):\n",
        "    \"\"\"Abstract base class for instruction-following judges.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    @abstractmethod\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"\n",
        "        Evaluate how many instructions were followed exactly.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of editing instructions to evaluate\n",
        "            original_paragraph: The original text before editing\n",
        "            completion: The edited text to evaluate\n",
        "\n",
        "        Returns:\n",
        "            Number of instructions followed exactly (0 to len(instructions))\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_evaluation_prompt(self, instructions: List[str], original_paragraph: str, completion: str) -> Tuple[str, str]:\n",
        "        \"\"\"Create system and user prompts for evaluation.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert evaluator of instruction-following in text editing tasks. \"\n",
        "            \"You will be given a set of editing instructions, the original paragraph, and a modified paragraph. \"\n",
        "            \"You must return ONLY the number of instructions that were followed exactly. \"\n",
        "            \"Do not explain or justify. Just return a single number (0 to N).\"\n",
        "        )\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "Instructions:\n",
        "{chr(10).join(f\"{i+1}. {instr}\" for i, instr in enumerate(instructions))}\n",
        "\n",
        "Original Paragraph:\n",
        "{original_paragraph}\n",
        "\n",
        "Edited Output:\n",
        "{completion}\n",
        "\n",
        "How many of the above {len(instructions)} instructions were followed exactly?\n",
        "Return just the number, nothing else.\n",
        "\"\"\"\n",
        "        return system_prompt, user_prompt\n",
        "\n",
        "    def _extract_score(self, response_text: str, max_instructions: int) -> int:\n",
        "        \"\"\"Extract numerical score from response text.\"\"\"\n",
        "        match = re.search(r'\\d+', response_text.strip())\n",
        "        if match:\n",
        "            value = int(match.group(0))\n",
        "            return max(0, min(value, max_instructions))\n",
        "        else:\n",
        "            print(f\"[{self.name}] Unexpected format: {response_text}\")\n",
        "            return 0\n"
      ],
      "metadata": {
        "id": "qO1fkjizaJog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class OpenAi"
      ],
      "metadata": {
        "id": "k-ZDHb3aaP1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenAIJudge(BaseJudge):\n",
        "    \"\"\"OpenAI-based instruction-following judge.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\"):\n",
        "        super().__init__(\"OpenAI\")\n",
        "        # Use provided key or get from APIKeyManager/environment\n",
        "        if api_key is None:\n",
        "            api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"OpenAI API key is required. Set it via parameter, environment variable OPENAI_API_KEY, or APIKeyManager.\")\n",
        "\n",
        "        self.client = AsyncOpenAI(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"Evaluate using OpenAI GPT model.\"\"\"\n",
        "        system_prompt, user_prompt = self._create_evaluation_prompt(instructions, original_paragraph, completion)\n",
        "\n",
        "        try:\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=50\n",
        "            )\n",
        "            reply = response.choices[0].message.content\n",
        "            return self._extract_score(reply, len(instructions))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error: {e}\")\n",
        "            return 0"
      ],
      "metadata": {
        "id": "ks9LwRONaUdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Claude"
      ],
      "metadata": {
        "id": "s298QaWSaYqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClaudeJudge(BaseJudge):\n",
        "    \"\"\"Claude-based instruction-following judge.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"claude-3-5-sonnet-20240620\"):\n",
        "        super().__init__(\"Claude\")\n",
        "        # Use provided key or get from environment\n",
        "        if api_key is None:\n",
        "            api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Claude API key is required. Set it via parameter, environment variable ANTHROPIC_API_KEY, or APIKeyManager.\")\n",
        "\n",
        "        self.client = anthropic.AsyncAnthropic(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    async def evaluate(self, instructions: List[str], original_paragraph: str, completion: str) -> int:\n",
        "        \"\"\"Evaluate using Claude model.\"\"\"\n",
        "        system_prompt, user_prompt = self._create_evaluation_prompt(instructions, original_paragraph, completion)\n",
        "\n",
        "        try:\n",
        "            response = await self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=50,\n",
        "                temperature=0,\n",
        "                system=system_prompt,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            reply = response.content[0].text\n",
        "            return self._extract_score(reply, len(instructions))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error: {e}\")\n",
        "            return 0"
      ],
      "metadata": {
        "id": "v1AhE9Qeaap1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class InstructionProcess"
      ],
      "metadata": {
        "id": "iWz01UY5ae5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionProcessor:\n",
        "    \"\"\"Utility class for processing instructions and prompts.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_instructions_and_paragraph(prompt: str) -> Tuple[List[str], str]:\n",
        "        \"\"\"\n",
        "        Extract instructions and paragraph from a formatted prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: The formatted prompt containing instructions and paragraph\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (instructions_list, paragraph)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If markers are not found or content is empty\n",
        "        \"\"\"\n",
        "        start_marker = \"Instructions:\"\n",
        "        end_marker = \"Return the final output as plain text with line breaks between paragraphs.\"\n",
        "\n",
        "        if start_marker not in prompt or end_marker not in prompt:\n",
        "            raise ValueError(\"Start or end marker not found in prompt.\")\n",
        "\n",
        "        # Find start and end positions\n",
        "        start_index = prompt.index(start_marker) + len(start_marker)\n",
        "        end_index = prompt.index(end_marker) + len(end_marker)\n",
        "\n",
        "        # Extract instruction block\n",
        "        instructions_block = prompt[start_index:prompt.index(end_marker)].strip()\n",
        "\n",
        "        # Split instructions into list by lines and remove empty ones\n",
        "        instructions_list = [line.strip() for line in instructions_block.splitlines() if line.strip()]\n",
        "\n",
        "        # Extract paragraph after end marker\n",
        "        paragraph = prompt[end_index:].strip()\n",
        "\n",
        "        if not instructions_list:\n",
        "            raise ValueError(\"No instructions found between markers.\")\n",
        "        if not paragraph:\n",
        "            raise ValueError(\"No paragraph found after the instructions block.\")\n",
        "\n",
        "        return instructions_list, paragraph\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_instructions(instructions: List[str], n_parts: int = 2) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Split instructions into approximately equal chunks.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of instructions to split\n",
        "            n_parts: Number of parts to split into\n",
        "\n",
        "        Returns:\n",
        "            List of instruction chunks\n",
        "        \"\"\"\n",
        "        total = len(instructions)\n",
        "        base_size = total // n_parts\n",
        "        remainder = total % n_parts\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        for i in range(n_parts):\n",
        "            # First 'remainder' chunks get one extra item\n",
        "            chunk_size = base_size + (1 if i < remainder else 0)\n",
        "            end = start + chunk_size\n",
        "            chunks.append(instructions[start:end])\n",
        "            start = end\n",
        "\n",
        "        return chunks"
      ],
      "metadata": {
        "id": "ITIbbN9iaiuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class InstructionFollowingEvaluator:"
      ],
      "metadata": {
        "id": "LnfqSAMHasLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionFollowingEvaluator:\n",
        "    \"\"\"\n",
        "    Main evaluator class that coordinates multiple judges to score instruction-following.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, judges: Optional[List[BaseJudge]] = None, api_key_manager: Optional[APIKeyManager] = None, n_chunks: int = 2):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            judges: List of judge instances to use for evaluation. If None, auto-creates based on available API keys.\n",
        "            api_key_manager: APIKeyManager instance for handling API keys\n",
        "            n_chunks: Number of chunks to split instructions into for parallel processing\n",
        "        \"\"\"\n",
        "        self.n_chunks = n_chunks\n",
        "        self.processor = InstructionProcessor()\n",
        "        self.api_key_manager = api_key_manager or APIKeyManager()\n",
        "\n",
        "        # Auto-configure from environment if no explicit manager provided\n",
        "        if api_key_manager is None:\n",
        "            self.api_key_manager.configure_from_env()\n",
        "\n",
        "        # Auto-create judges if none provided\n",
        "        if judges is None:\n",
        "            self.judges = self._create_default_judges()\n",
        "        else:\n",
        "            self.judges = judges\n",
        "\n",
        "    def _create_default_judges(self) -> List[BaseJudge]:\n",
        "        \"\"\"Create judges based on available API keys.\"\"\"\n",
        "        judges = []\n",
        "\n",
        "        if self.api_key_manager.is_openai_configured():\n",
        "            try:\n",
        "                judges.append(OpenAIJudge(api_key=self.api_key_manager.get_openai_key()))\n",
        "            except ValueError as e:\n",
        "                print(f\"Failed to create OpenAI judge: {e}\")\n",
        "\n",
        "        if self.api_key_manager.is_claude_configured():\n",
        "            try:\n",
        "                judges.append(ClaudeJudge(api_key=self.api_key_manager.get_claude_key()))\n",
        "            except ValueError as e:\n",
        "                print(f\"Failed to create Claude judge: {e}\")\n",
        "\n",
        "        if not judges:\n",
        "            raise ValueError(\"No judges could be created. Please configure API keys.\")\n",
        "\n",
        "        return judges\n",
        "\n",
        "    def add_judge(self, judge: BaseJudge) -> None:\n",
        "        \"\"\"Add a judge to the evaluator.\"\"\"\n",
        "        self.judges.append(judge)\n",
        "\n",
        "    def remove_judge(self, judge_name: str) -> bool:\n",
        "        \"\"\"Remove a judge by name. Returns True if removed, False if not found.\"\"\"\n",
        "        for i, judge in enumerate(self.judges):\n",
        "            if judge.name == judge_name:\n",
        "                del self.judges[i]\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_judge_names(self) -> List[str]:\n",
        "        \"\"\"Get names of all configured judges.\"\"\"\n",
        "        return [judge.name for judge in self.judges]\n",
        "\n",
        "    async def _evaluate_single_completion(\n",
        "        self,\n",
        "        instructions: List[str],\n",
        "        paragraph: str,\n",
        "        completion: str\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        Evaluate a single completion using all judges.\n",
        "\n",
        "        Args:\n",
        "            instructions: List of instructions to evaluate against\n",
        "            paragraph: Original paragraph\n",
        "            completion: Completion to evaluate\n",
        "\n",
        "        Returns:\n",
        "            List of scores from each judge (normalized 0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        # Split instructions into chunks for parallel processing\n",
        "        instruction_chunks = self.processor.chunk_instructions(instructions, self.n_chunks)\n",
        "        total_instructions = len(instructions)\n",
        "\n",
        "        judge_scores = []\n",
        "\n",
        "        # Evaluate with each judge\n",
        "        for judge in self.judges:\n",
        "            # Process chunks in parallel for this judge\n",
        "            chunk_tasks = [\n",
        "                judge.evaluate(chunk, paragraph, completion)\n",
        "                for chunk in instruction_chunks\n",
        "            ]\n",
        "\n",
        "            chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)\n",
        "\n",
        "            # Sum up followed instructions across chunks\n",
        "            total_followed = 0\n",
        "            for result in chunk_results:\n",
        "                if isinstance(result, Exception):\n",
        "                    print(f\"[{judge.name}] Chunk evaluation failed: {result}\")\n",
        "                    continue\n",
        "                total_followed += result\n",
        "\n",
        "            # Normalize score\n",
        "            normalized_score = total_followed / total_instructions if total_instructions > 0 else 0.0\n",
        "            judge_scores.append(normalized_score)\n",
        "\n",
        "        return judge_scores\n",
        "\n",
        "    async def score(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        completions: List[str],\n",
        "        models: List[str]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Score prompt-completion pairs using multiple judges.\n",
        "\n",
        "        Args:\n",
        "            prompts: List of full prompts (including instructions + paragraph)\n",
        "            completions: List of completions corresponding to the prompts\n",
        "            models: List of model identifiers for each completion\n",
        "\n",
        "        Returns:\n",
        "            Dictionary: model_id → average compliance score (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        if not (len(prompts) == len(completions) == len(models)):\n",
        "            raise ValueError(\"Input lists must be equal length\")\n",
        "\n",
        "        if not prompts:\n",
        "            return {}\n",
        "\n",
        "        # Extract instructions and paragraph from first prompt (assuming all prompts have same format)\n",
        "        try:\n",
        "            instructions, paragraph = self.processor.extract_instructions_and_paragraph(prompts[0])\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Failed to parse prompt format: {e}\")\n",
        "\n",
        "        model_scores = defaultdict(list)\n",
        "\n",
        "        # Create tasks for all completions\n",
        "        evaluation_tasks = []\n",
        "        for i, (prompt, completion, model_id) in enumerate(zip(prompts, completions, models)):\n",
        "            task = self._evaluate_single_completion(instructions, paragraph, completion)\n",
        "            evaluation_tasks.append((task, model_id))\n",
        "\n",
        "        # Execute all evaluations concurrently\n",
        "        for task, model_id in evaluation_tasks:\n",
        "            try:\n",
        "                judge_scores = await task\n",
        "\n",
        "                # Average across judges for this completion\n",
        "                if judge_scores:\n",
        "                    final_score = sum(judge_scores) / len(judge_scores)\n",
        "                    model_scores[model_id].append(final_score)\n",
        "                else:\n",
        "                    print(f\"No valid scores for model {model_id}\")\n",
        "                    model_scores[model_id].append(0.0)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation failed for model {model_id}: {e}\")\n",
        "                model_scores[model_id].append(0.0)\n",
        "\n",
        "        # Average scores per model\n",
        "        avg_scores = {\n",
        "            model_id: sum(scores) / len(scores) if scores else 0.0\n",
        "            for model_id, scores in model_scores.items()\n",
        "        }\n",
        "\n",
        "        return avg_scores"
      ],
      "metadata": {
        "id": "sgO0kNL-bZ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_3G_YOnVeEnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convienice and Helping Functions"
      ],
      "metadata": {
        "id": "OHsLsMjUeEoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def create_evaluator(\n",
        "    openai_api_key: Optional[str] = None,\n",
        "    claude_api_key: Optional[str] = None,\n",
        "    use_openai: bool = True,\n",
        "    use_claude: bool = True,\n",
        "    config_file: Optional[str] = None,\n",
        "    api_config: Optional[Dict[str, str]] = None\n",
        ") -> InstructionFollowingEvaluator:\n",
        "    \"\"\"\n",
        "    Create an evaluator with specified judges and flexible API key configuration.\n",
        "\n",
        "    Args:\n",
        "        openai_api_key: OpenAI API key (overrides other sources)\n",
        "        claude_api_key: Claude API key (overrides other sources)\n",
        "        use_openai: Whether to include OpenAI judge\n",
        "        use_claude: Whether to include Claude judge\n",
        "        config_file: Path to JSON config file with API keys\n",
        "        api_config: Dictionary with API key configuration\n",
        "\n",
        "    Returns:\n",
        "        Configured InstructionFollowingEvaluator instance\n",
        "    \"\"\"\n",
        "    # Setup API key manager\n",
        "    api_manager = APIKeyManager()\n",
        "\n",
        "\n",
        "    # Direct API key parameters take highest priority\n",
        "    if openai_api_key:\n",
        "        api_manager.set_openai_key(openai_api_key)\n",
        "    if claude_api_key:\n",
        "        api_manager.set_claude_key(claude_api_key)\n",
        "\n",
        "\n",
        "\n",
        "    # Create judges based on preferences and availability\n",
        "    judges = []\n",
        "\n",
        "    if use_openai and api_manager.is_openai_configured():\n",
        "        try:\n",
        "            judges.append(OpenAIJudge(api_key=api_manager.get_openai_key()))\n",
        "        except ValueError as e:\n",
        "            print(f\"OpenAI judge creation failed: {e}\")\n",
        "\n",
        "    if use_claude and api_manager.is_claude_configured():\n",
        "        try:\n",
        "            judges.append(ClaudeJudge(api_key=api_manager.get_claude_key()))\n",
        "        except ValueError as e:\n",
        "            print(f\"Claude judge creation failed: {e}\")\n",
        "\n",
        "    if not judges:\n",
        "        available = api_manager.get_available_services()\n",
        "        raise ValueError(f\"No judges could be created. Available services: {available}\")\n",
        "\n",
        "    return InstructionFollowingEvaluator(judges, api_manager)\n",
        "\n",
        "\n",
        "def create_api_manager() -> APIKeyManager:\n",
        "    \"\"\"Create and return a new APIKeyManager instance.\"\"\"\n",
        "    return APIKeyManager()\n",
        "\n",
        "\n",
        "async def evaluate_completions(\n",
        "    prompts: List[str],\n",
        "    completions: List[str],\n",
        "    models: List[str],\n",
        "    openai_api_key: Optional[str] = None,\n",
        "    claude_api_key: Optional[str] = None,\n",
        "    config_file: Optional[str] = None\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Convenience function to evaluate completions with flexible API key configuration.\n",
        "\n",
        "    Args:\n",
        "        prompts: List of prompts\n",
        "        completions: List of completions\n",
        "        models: List of model identifiers\n",
        "        openai_api_key: OpenAI API key\n",
        "        claude_api_key: Claude API key\n",
        "        config_file: Path to config file with API keys\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of model scores\n",
        "    \"\"\"\n",
        "    evaluator = await create_evaluator(\n",
        "        openai_api_key=openai_api_key,\n",
        "        claude_api_key=claude_api_key,\n",
        "        config_file=config_file\n",
        "    )\n",
        "\n",
        "    return await evaluator.score(prompts, completions, models)"
      ],
      "metadata": {
        "id": "PhiClPdJfM8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "XOD-gEbCfOrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data\n",
        "import re\n",
        "from typing import List\n",
        "def read_prompt_as_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "\n",
        "     return f.read()"
      ],
      "metadata": {
        "id": "KqJPz_byfhA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_completions(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    completions = re.findall(r\"Completion \\d+:\\s*(.*?)\\s*(?=Completion \\d+:|$)\", text, re.DOTALL)\n",
        "    return [c.strip() for c in completions]\n",
        "\n",
        "def load_models(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        models = [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return models\n"
      ],
      "metadata": {
        "id": "-NvF6Tsrkt99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = read_prompt_as_text(\"/content/Prompts.txt\")\n",
        "completions = load_completions(\"/content/Completions.txt\")\n",
        "model_ids = load_models(\"/content/Models.txt\")"
      ],
      "metadata": {
        "id": "yYBnrnxSk0Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Sample data\n",
        "    prompt = read_prompt_as_text(\"/content/Prompts.txt\")\n",
        "    completions = load_completions(\"/content/Completions.txt\")\n",
        "    models = load_models(\"/content/Models.txt\")\n",
        "    prompts = [prompt] * len(completions)\n",
        "\n",
        "    # Method 1: Direct API key configuration\n",
        "    evaluator1 = await create_evaluator(\n",
        "        openai_api_key=\"\",\n",
        "        claude_api_key=\"\" )\n",
        "     # Get scores using any evaluator\n",
        "    scores = await evaluator1.score(prompts, completions, models)\n",
        "\n",
        "    print(\"Model Scores:\")\n",
        "    for model_id, score in scores.items():\n",
        "        print(f\"{model_id}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "wcLAYJ6nfRCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB1NSTtql3z-",
        "outputId": "a33af669-a216-47ce-8912-0ad07644dd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Scores:\n",
            "Gemini: 0.536\n",
            "Mistral: 0.500\n",
            "OpenAi: 0.536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hfUGTSfKlfYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X5_UHaMZsey_"
      }
    }
  ]
}