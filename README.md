# LLM-as-a-Judge

## Overview

This repository contains the code for evaluating model responses based on instruction-based prompts.

Each prompt includes an instruction for editing a given paragraph.  
The evaluation process takes the following inputs:

- `prompt`
- `completion`
- `model_id`

## Versions

The repository includes four different evaluation modes:

1. **OpenAI Synchronous**
2. **Jury of Judges Synchronous**
3. **Asynchronous**
4. **Batch Asynchronous**

## Features

- Includes sample prompts, completions, and model IDs.
- Modular and extensible for custom evaluation workflows.

## Contribution

Feel free to explore the codebase and add more functions or improvements. Contributions are welcome!
